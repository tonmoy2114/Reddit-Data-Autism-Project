{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim.corpora import Dictionary\n",
    "from stop_words import get_stop_words\n",
    "from gensim.models import Phrases\n",
    "from pprint import pprint\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import HdpModel\n",
    "import gensim\n",
    "import copy\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import pyLDAvis.gensim as gensimvis\n",
    "import pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program Started at: 2018-02-28 00:49:43.547462\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'postCommentCombined.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-66cd93c3a27d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Program Started at: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'postCommentCombined.csv'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsvLayerFile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mcsvReader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvLayerFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'postCommentCombined.csv'"
     ]
    }
   ],
   "source": [
    "writeFileTokens = open('output/post_comment_tokens.csv', 'w+')\n",
    "writeFileStopWordRemoved = open('output/post_comment_after_stopwords.csv', 'w+')\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "docs = []\n",
    "\n",
    "remove_word_list = ['http','https', 'www','ve','er','nt', 'can', 're', 'com']\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "print(\"Program Started at: \" + str(datetime.now()))\n",
    "\n",
    "with open('postCommentCombined.csv') as csvLayerFile:\n",
    "    csvReader = csv.reader(csvLayerFile)\n",
    "    i = 0\n",
    "    for row in csvReader:\n",
    "        mStr = \"\"\n",
    "        for item in row:\n",
    "            mStr+=item +\"\\n\"\n",
    "        docs.append(mStr)\n",
    "\n",
    "for idx in range(len(docs)):\n",
    "    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "    #remove word mentioned in remove_word_list\n",
    "    docs[idx] = [x for x in docs[idx] if x not in remove_word_list]\n",
    "\n",
    "    writeFileTokens.write(str(docs[idx])[1:-1])\n",
    "    writeFileTokens.write(\"\\n\")\n",
    "    #print(docs[idx])\n",
    "    # remove stop words from tokens\n",
    "    docs[idx] = [i for i in docs[idx] if not i in en_stop]\n",
    "    writeFileStopWordRemoved.write(str(docs[idx])[1:-1])\n",
    "    writeFileStopWordRemoved.write(\"\\n\")\n",
    "\n",
    "writeFileTokens.close()\n",
    "writeFileStopWordRemoved.close()\n",
    "\n",
    "# Remove numbers, but not words that contain numbers.\n",
    "docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "# Remove words that are only one character.\n",
    "docs = [[token for token in doc if len(token) > 1] for doc in docs]\n",
    "\n",
    "mfile = open('output/post_after_numbers_1charremoved.csv', 'w+')\n",
    "for doc in docs:\n",
    "    mfile.write(str(doc)[1:-1])\n",
    "    mfile.write(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Lemmatize the documents.\n",
    "\n",
    "\n",
    "# Lemmatize all words in documents.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "\n",
    "mfile = open('output/post_comment_after_lemmatization.csv', 'w+')\n",
    "for doc in docs:\n",
    "    mfile.write(str(doc)[1:-1])\n",
    "    mfile.write(\"\\n\")\n",
    "# Compute bigrams.\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 15 times or more).\n",
    "bigram = Phrases(docs, min_count=15)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)\n",
    "\n",
    "# Remove rare and common tokens.\n",
    "mfile = open('output/post_comment_after_bigram.csv', 'w+')\n",
    "for doc in docs:\n",
    "    mfile.write(str(doc)[1:-1])\n",
    "    mfile.write(\"\\n\")\n",
    "\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.6)\n",
    "\n",
    "\n",
    "# Vectorize data.\n",
    "\n",
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LDA model.\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 10\n",
    "chunksize = 2000\n",
    "passes = 3\n",
    "iterations = 40\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "'''\n",
    "#model = LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize, \\\n",
    "                       alpha='auto', eta='auto', \\\n",
    "                       iterations=iterations, num_topics=num_topics, \\\n",
    "                       passes=passes, eval_every=eval_every)\n",
    "'''\n",
    "\n",
    "#model = LdaModel(corpus, num_topics=num_topics, id2word = id2word, passes=passes)\n",
    "#model.save('reddit_autism_pass3_topic10_lda.model')\n",
    "\n",
    "hdp = HdpModel(corpus, dictionary)\n",
    "hdp.save('reddit_autism_hdp.model')\n",
    "\n",
    "print(model.print_topics(num_topics=num_topics, num_words=12))\n",
    "print(\"Program Ended at: \" + str(datetime.now()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
